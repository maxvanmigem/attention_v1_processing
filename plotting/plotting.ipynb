{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import mne, os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from mne.datasets import somato\n",
    "from mne.time_frequency import tfr_morlet\n",
    "%matplotlib qt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_01-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4189 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_02-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4196 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_03-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 32 columns\n",
      "4188 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_04-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_05-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4054 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_06-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_07-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "3918 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_08-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "3506 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_09-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "3425 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_10-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_11-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4195 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_12-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4194 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_13-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_14-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4190 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_15-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4199 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_16-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_17-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_18-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4195 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_19-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4197 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n",
      "Reading c:\\Users\\mvmigem\\Documents\\data\\project_1\\preprocessed\\mastoid_ref\\main_eventset_mastoidref_21-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 33 columns\n",
      "4200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvmigem\\AppData\\Local\\Temp\\ipykernel_22316\\2384206660.py:37: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  eps = mne.concatenate_epochs(ep_list).crop(tmin=epoch_tmin,tmax=epoch_tmax).apply_baseline()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 35 columns\n",
      "82046 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Load data\n",
    "\"\"\"\n",
    "# Big list\n",
    "ep_list = []\n",
    "# Sup grouped list\n",
    "ep_group1 = []\n",
    "ep_group2 = []\n",
    "ep_group3 = []\n",
    "ep_group4 = []\n",
    "subjects = []\n",
    "# Specify the epoch length we will be looking at\n",
    "epoch_tmin = -0.1\n",
    "epoch_tmax = 0.5\n",
    "\n",
    "for sub in range(1,22):\n",
    "\n",
    "    if sub == 20:\n",
    "        continue\n",
    "    cleaned_data_dir = '/Users/mvmigem/Documents/data/project_1/preprocessed/mastoid_ref/'\n",
    "    clean_epoch_path = os.path.join(cleaned_data_dir,f'main_eventset_mastoidref_{sub:02}-epo.fif')\n",
    "    epochs = mne.read_epochs(clean_epoch_path)\n",
    "    epochs.info['bads']= []\n",
    "    ep_list.append(epochs)\n",
    "\n",
    "    # if epochs.metadata['loc_quad'].iloc[0] == 0:\n",
    "    #     ep_group1.append(epochs)\n",
    "    # elif epochs.metadata['loc_quad'].iloc[0] == 1:\n",
    "    #     ep_group2.append(epochs)\n",
    "    # elif epochs.metadata['loc_quad'].iloc[0] == 2:\n",
    "    #     ep_group3.append(epochs)\n",
    "    # elif epochs.metadata['loc_quad'].iloc[0] == 3:\n",
    "    #     ep_group4.append(epochs)\n",
    "    print(epochs.info['nchan'])\n",
    "\n",
    "# Concat list into big epoch object\n",
    "eps = mne.concatenate_epochs(ep_list).crop(tmin=epoch_tmin,tmax=epoch_tmax).apply_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub_list = eps.metadata['participant'].unique() \n",
    "\n",
    "# Divide the epoch file into sections based on metadata that can't be distinguished by event names\n",
    "# Drop catch trials\n",
    "epochs_nocatch = eps.copy()['catch_trial == 0']\n",
    "# # Divide attention conditions\n",
    "# epochs_attended = epochs_nocatch['attention == \"attended\"']\n",
    "# epochs_unattended = epochs_nocatch['attention == \"unattended\"']\n",
    "# # Divide by staring position\n",
    "# epochs_start3 = epochs_nocatch['start_position == 2']\n",
    "# epochs_start1 = epochs_nocatch['start_position == 0']\n",
    "# epochs_start2 = epochs_nocatch['start_position == 1']\n",
    "# epochs_start4 = epochs_nocatch['start_position == 3']\n",
    "# # Divide unattended trials by start pos\n",
    "# epochs_unattended_start3 = epochs_unattended['start_position == 2']\n",
    "# epochs_unattended_start1 = epochs_unattended['start_position == 0']\n",
    "# epochs_unattended_start2 = epochs_unattended['start_position == 1']\n",
    "# epochs_unattended_start4 = epochs_unattended['start_position == 3']\n",
    "# # Divide attented trials by start pos\n",
    "# epochs_attended_start3 = epochs_attended['start_position == 2']\n",
    "# epochs_attended_start1 = epochs_attended['start_position == 0']\n",
    "# epochs_attended_start2 = epochs_attended['start_position == 1']\n",
    "# epochs_attended_start4 = epochs_attended['start_position == 3']\n",
    "# # Extra devision for the odd boys\n",
    "# epochs_attended_reg = epochs_attended['expected ==\"regular\"']\n",
    "# epochs_attended_odd = epochs_attended['expected == \"odd\"']\n",
    "# epochs_unattended_reg = epochs_unattended['expected ==\"regular\"']\n",
    "# epochs_unattended_odd = epochs_unattended['expected == \"odd\"']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evoked plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main pos individual channels\n",
    "\"\"\"\n",
    "evoked_pos1_list = []\n",
    "evoked_pos2_list = []\n",
    "evoked_pos3_list = []\n",
    "evoked_pos4_list = []\n",
    "for i,sub in enumerate(sub_list):\n",
    "    evoked_pos1_list.append(epochs_nocatch['pos1'][f'participant == {sub}'].average())\n",
    "    evoked_pos2_list.append(epochs_nocatch['pos2'][f'participant == {sub}'].average())\n",
    "    evoked_pos3_list.append(epochs_nocatch['pos3'][f'participant == {sub}'].average())\n",
    "    evoked_pos4_list.append(epochs_nocatch['pos4'][f'participant == {sub}'].average())\n",
    "\n",
    "evoked_pos1 = mne.grand_average(evoked_pos1_list)\n",
    "evoked_pos2 = mne.grand_average(evoked_pos2_list)\n",
    "evoked_pos3 = mne.grand_average(evoked_pos3_list)\n",
    "evoked_pos4 = mne.grand_average(evoked_pos4_list)\n",
    "\n",
    "evokeds_list = [evoked_pos1,evoked_pos2,evoked_pos3,evoked_pos4]\n",
    "conds = ('pos1','pos2','pos3','pos4')\n",
    "# conds = ('seq1','seq2','seq3','seq4')\n",
    "\n",
    "norm = dict(zip(conds, evokeds_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event selection for prediction effects\n",
    "\n",
    "# Remove catch\n",
    "# epochs_nocatch = eps.copy()['catch_trial == 0']['pos4']\n",
    "# Select second stim of trial\n",
    "epochs_stim2 = epochs_nocatch['seq2']\n",
    "# Regular vs odd\n",
    "epochs_odd = epochs_stim2['expected == \"odd\"']\n",
    "epochs_reg = epochs_stim2['expected == \"regular\"']['precedes_odd == 1']\n",
    "# Attention\n",
    "ep_odd_att = epochs_odd['attention == \"attended\"']\n",
    "ep_odd_unatt = epochs_odd['attention == \"unattended\"']\n",
    "ep_reg_att = epochs_reg['attention == \"attended\"']\n",
    "ep_reg_unatt = epochs_reg['attention == \"unattended\"']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_meta = epochs_nocatch['seq2'].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "checking prediction manipulation on p3, means agragating across positions \n",
    "\"\"\"\n",
    "att_reg_ev = []\n",
    "att_odd_ev = []\n",
    "unatt_reg_ev = []\n",
    "unatt_odd_ev = []\n",
    "# totals_grand = []\n",
    "\n",
    "\n",
    "for i, sub in enumerate(sub_list):\n",
    "    \n",
    "    att_reg_ev.append(\n",
    "        epochs_nocatch['seq2'][f\"participant == {sub}\"][\"attention == 'attended'\"]['expected == \"regular\"']['precedes_odd == 1'].average())\n",
    "    att_odd_ev.append(\n",
    "        epochs_nocatch['seq2'][f\"participant == {sub}\"][\"attention == 'attended'\"]['expected == \"odd\"'].average())\n",
    "    unatt_reg_ev.append(\n",
    "        epochs_nocatch['seq2'][f\"participant == {sub}\"][\"attention == 'unattended'\"]['expected == \"regular\"']['precedes_odd == 1'].average())\n",
    "    unatt_odd_ev.append(\n",
    "        epochs_nocatch['seq2'][f\"participant == {sub}\"][\"attention == 'unattended'\"]['expected == \"odd\"'].average())\n",
    "    \n",
    "    # totals_grand.append(\n",
    "    #     epochs_nocatch['seq2'].average())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evoked_pred = [mne.grand_average(att_reg_ev),\n",
    "               mne.grand_average(att_odd_ev),\n",
    "               mne.grand_average(unatt_reg_ev),\n",
    "               mne.grand_average(unatt_odd_ev),]\n",
    "\n",
    "conditions = ('attended regular','attended odd','unattended regular','unattended odd')\n",
    "\n",
    "norm = dict(zip(conditions, evoked_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "create df with the neededed values for p3 analysis\n",
    "\"\"\"\n",
    "# init df\n",
    "long_sub_list = [item for item in sub_list for _ in range(4)]\n",
    "p3_df = pd.DataFrame(long_sub_list, columns=['subject'])\n",
    "\n",
    "# selected channels\n",
    "roi = ['P1','Pz','P2','CP1','CPz','CP2','C1','Cz','C2']\n",
    "roi_ix = mne.pick_channels(epochs_nocatch[0].info[\"ch_names\"], include=roi)\n",
    "roi_dict = dict(centropar = list(roi_ix))\n",
    "p3_tmin, p3_tmax = 0.25, 0.45\n",
    "\n",
    "# loop subjects\n",
    "mid_att = []\n",
    "mid_pred = []\n",
    "mid_peak = []\n",
    "mid_lat = []\n",
    "mid_mean = []\n",
    "sub_check = []\n",
    "for i, sub in enumerate(sub_list):\n",
    "    print(sub)\n",
    "    # attended and regular\n",
    "    ev_roi = mne.channels.combine_channels(inst = att_reg_ev[i], groups= roi_dict)\n",
    "    mean_amp = ev_roi.crop(tmin=p3_tmin,tmax=p3_tmax).data.mean(axis=1) * 1e6\n",
    "    mid_att.append('attended')\n",
    "    mid_pred.append('regular')\n",
    "    mid_mean.append(mean_amp[0])\n",
    "    sub_check.append(sub)\n",
    "    # attended and odd\n",
    "    ev_roi = mne.channels.combine_channels(inst = att_odd_ev[i], groups= roi_dict)\n",
    "    mean_amp = ev_roi.crop(tmin=p3_tmin,tmax=p3_tmax).data.mean(axis=1) * 1e6\n",
    "    mid_att.append('attended')\n",
    "    mid_pred.append('odd')\n",
    "    mid_mean.append(mean_amp[0])\n",
    "    sub_check.append(sub)\n",
    "    # unattended and regular\n",
    "    ev_roi = mne.channels.combine_channels(inst = unatt_reg_ev[i], groups= roi_dict)\n",
    "    mean_amp = ev_roi.crop(tmin=p3_tmin,tmax=p3_tmax).data.mean(axis=1) * 1e6\n",
    "    mid_att.append('unattended')\n",
    "    mid_pred.append('regular')\n",
    "    mid_mean.append(mean_amp[0])\n",
    "    sub_check.append(sub)\n",
    "    # unattended and odd\n",
    "    ev_roi = mne.channels.combine_channels(inst = unatt_odd_ev[i], groups= roi_dict)\n",
    "    mean_amp = ev_roi.crop(tmin=p3_tmin,tmax=p3_tmax).data.mean(axis=1) * 1e6\n",
    "    mid_att.append('unattended')\n",
    "    mid_pred.append('odd')\n",
    "    mid_mean.append(mean_amp[0])\n",
    "    sub_check.append(sub)\n",
    "\n",
    "p3_df['attention'] = mid_att\n",
    "p3_df['expectation'] = mid_pred\n",
    "p3_df['mean_amp'] = mid_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_df.to_csv(r'C:\\Users\\mvmigem\\Documents\\data\\project_1\\compiled_dataframes\\p3_df.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main effects \n",
    "\"\"\"\n",
    "reg_att_up = []\n",
    "odd_att_up = []\n",
    "reg_unatt_up = []\n",
    "odd_unatt_up = []\n",
    "\n",
    "reg_att_down = []\n",
    "odd_att_down = []\n",
    "reg_unatt_down = []\n",
    "odd_unatt_down = []\n",
    "\n",
    "cond_ep_lists = [ep_reg_att,ep_odd_att,ep_reg_unatt,ep_odd_unatt] \n",
    "up_conds = [reg_att_up,odd_att_up,reg_unatt_up,odd_unatt_up]  \n",
    "down_conds = [reg_att_down,odd_att_down,reg_unatt_down,odd_unatt_down]  \n",
    "ev_names = ['reg_att','odd_att','reg_unatt','odd_unatt']\n",
    "# List to know which position was used to select the up or down later\n",
    "sub_pos_info = []\n",
    "\n",
    "evokeds_up = dict(zip(ev_names,up_conds))\n",
    "evokeds_down = dict(zip(ev_names,down_conds))\n",
    "\n",
    "for i, sub in enumerate(sub_list):\n",
    "    # which set of positions does this sub have\n",
    "    loc_quad_values = ep_reg_att[f'participant == {sub}'].metadata[\"loc_quad\"]\n",
    "    loc_quad_value = loc_quad_values.unique()[0]\n",
    "    # Dict to stor positional info\n",
    "    sub_info_dict = {}\n",
    "    sub_info_dict['subject'] = sub\n",
    "    # Positions 1 and 3\n",
    "    if loc_quad_value == 0  or loc_quad_value== 2 :\n",
    "        # make dict to store the selected evokeds\n",
    "        ev_position1 = {}\n",
    "        # for every condition\n",
    "        for i, epoch in enumerate(cond_ep_lists):\n",
    "            # transform the epochs into evokeds\n",
    "            ev_position1[ev_names[i]] = epoch['pos1'][f'participant == {sub}'].average()\n",
    "        # same for position 3\n",
    "        ev_position3 = {}\n",
    "        for i, epoch in enumerate(cond_ep_lists):\n",
    "            ev_position3[ev_names[i]] = epoch['pos3'][f'participant == {sub}'].average()\n",
    "        # for every condition in the upper vf\n",
    "        for cond,evoked in ev_position1.items():\n",
    "            evokeds_up[cond].append(evoked)\n",
    "        # same for down\n",
    "        for cond,evoked in ev_position3.items():\n",
    "            evokeds_down[cond].append(evoked)\n",
    "        # Store position information\n",
    "        sub_info_dict['up_pos'] = 1\n",
    "        sub_info_dict['down_pos'] = 3\n",
    "    # same for positions 2 and 4\n",
    "    if loc_quad_value == 1  or loc_quad_value== 3 :\n",
    "        ev_position2 = {}\n",
    "        for i, evoked in enumerate(cond_ep_lists):\n",
    "            ev_position2[ev_names[i]] = evoked['pos2'][f'participant == {sub}'].average()\n",
    "        ev_position4 = {}\n",
    "        for i, evoked in enumerate(cond_ep_lists):\n",
    "            ev_position4[ev_names[i]] = evoked['pos4'][f'participant == {sub}'].average()\n",
    "        # for every condition in the upper vf\n",
    "        for cond,evoked in ev_position2.items():\n",
    "            # append to that conditions list in the up_dict\n",
    "            evokeds_up[cond].append(evoked)\n",
    "        # same for lower vf\n",
    "        for cond,evoked in ev_position4.items():\n",
    "            evokeds_down[cond].append(evoked)\n",
    "        sub_info_dict['up_pos'] = 2\n",
    "        sub_info_dict['down_pos'] = 4\n",
    "    # Add info_dict_to list\n",
    "    sub_pos_info.append(sub_info_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load peak properties of localiser data\n",
    "peak_properties = pd.read_csv(r'C:\\Users\\mvmigem\\Documents\\data\\project_1\\compiled_dataframes\\c1_peak_properties.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfrom dict of lists to list of dicts for plotting individually\n",
    "subj_uplist = [dict(zip(evokeds_up, values)) for values in zip(*evokeds_up.values())]\n",
    "subj_downlist = [dict(zip(evokeds_down, values)) for values in zip(*evokeds_down.values())]\n",
    "scale = [-12,12]\n",
    "# for i in range(20):\n",
    "#     the_row = peak_properties[peak_properties['subject'] == sub_pos_info[i]['subject']]\n",
    "#     pos = sub_pos_info[i]['down_pos']\n",
    "#     channel = the_row[f'pos{pos}_peak_channel'].iloc[0]\n",
    "#     mne.viz.plot_compare_evokeds(subj_downlist[i], picks= channel, vlines=[0.05,0.1],ylim=dict(eeg=scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes for export and analysis\n",
    "long_sub_list = [item for item in sub_list for _ in range(4)]\n",
    "c1_long_df = pd.DataFrame(long_sub_list, columns=['subject'])\n",
    "window_halfwidth = 0.01\n",
    "# Condition columns\n",
    "mid_att = []\n",
    "mid_pred = []\n",
    "# General analysis list init \n",
    "genup_amp = []\n",
    "gendown_amp = []\n",
    "general_amp = []\n",
    "# General time window estimate\n",
    "gen_lat = peak_properties['grand_average_peak_latency'][0]\n",
    "gen_tmin = gen_lat - window_halfwidth\n",
    "gen_tmax = gen_lat + window_halfwidth\n",
    "# \n",
    "selectup_gen_evoked = []\n",
    "selectdown_gen_evoked = []\n",
    "\n",
    "# Tailored analysis empty lists\n",
    "tailup_amp = []\n",
    "taildown_amp = []\n",
    "tailored_amp =[]\n",
    "\n",
    "selectup_tail_evoked = []\n",
    "selectdown_tail_evoked = []\n",
    "\n",
    "# Ultra tailored analysis values \n",
    "ultraup_amp = []\n",
    "ultradown_amp = []\n",
    "ultra_amp = []\n",
    "\n",
    "selectup_ultra_evoked = []\n",
    "selectdown_ultra_evoked = []\n",
    "\n",
    "conditions_att = ['attended','attended','unattended','unattended',]\n",
    "conditions_pred = ['regular','odd','regular','odd',]\n",
    "conditions = ['reg_att','odd_att','reg_unatt','odd_unatt']\n",
    "\n",
    "# Static C1 window analysis\n",
    "for i, sub in enumerate(sub_list):\n",
    "    # Dicts to store the selected evoked\n",
    "    upgen_select_dict = {}\n",
    "    downgen_select_dict = {}\n",
    "    uptail_select_dict = {}\n",
    "    downtail_select_dict = {}\n",
    "    upultra_select_dict = {}\n",
    "    downultra_select_dict = {}\n",
    "    # Select the channel and latency based on localiser info\n",
    "    sub_row = peak_properties[peak_properties['subject'] == sub_pos_info[i]['subject']]\n",
    "    # Tailored info\n",
    "    tailored_channel = sub_row['all_pos_peak_channel'].iloc[0]\n",
    "    tailored_latency = sub_row['all_pos_peak_latency'].iloc[0]\n",
    "    tail_tmin = tailored_latency - window_halfwidth\n",
    "    tail_tmax = tailored_latency + window_halfwidth\n",
    "    # Ultra tailored info\n",
    "    up_pos = sub_pos_info[i]['up_pos']\n",
    "    down_pos = sub_pos_info[i]['down_pos']\n",
    "    ultraup_channel = sub_row[f'pos{up_pos}_peak_channel'].iloc[0]\n",
    "    ultradown_channel = sub_row[f'pos{down_pos}_peak_channel'].iloc[0]\n",
    "    ultraup_latency = sub_row[f'pos{up_pos}_peak_latency'].iloc[0]\n",
    "    ultradown_latency = sub_row[f'pos{down_pos}_peak_latency'].iloc[0]\n",
    "    ultraup_tmin = ultraup_latency - window_halfwidth\n",
    "    ultraup_tmax = ultraup_latency + window_halfwidth\n",
    "    ultradown_tmin = ultradown_latency - window_halfwidth\n",
    "    ultradown_tmax = ultradown_latency + window_halfwidth\n",
    "\n",
    "    for ind in range(4):\n",
    "        # attended and regular\n",
    "        mid_att.append(conditions_att[ind])\n",
    "        mid_pred.append(conditions_pred[ind])\n",
    "        # General\n",
    "        # Pick channel\n",
    "        gen_selected_up = subj_uplist[i][conditions[ind]].copy().pick(['POz']) \n",
    "        gen_selected_down = subj_downlist[i][conditions[ind]].copy().pick(['POz'])\n",
    "        upgen_select_dict[conditions[ind]] =  gen_selected_up\n",
    "        downgen_select_dict[conditions[ind]] =  gen_selected_down\n",
    "        # Extract data\n",
    "        gen_up_mean_data = gen_selected_up.copy().crop(tmin=gen_tmin,tmax=gen_tmax).data\n",
    "        gen_down_mean_data = gen_selected_down.copy().crop(tmin=gen_tmin,tmax=gen_tmax).data\n",
    "        # Agragate and append\n",
    "        genup_mean = gen_up_mean_data.mean(axis=1) * 1e6\n",
    "        gendown_mean = gen_down_mean_data.mean(axis=1) * 1e6\n",
    "        genup_amp.append(genup_mean[0])\n",
    "        gendown_amp.append(gendown_mean[0])\n",
    "        # RMS\n",
    "        gen_mean = np.mean([np.sqrt(genup_mean**2),np.sqrt(gendown_mean**2)])\n",
    "        general_amp.append(gen_mean)\n",
    "\n",
    "        # Tailored\n",
    "        # Pick channel\n",
    "        tail_selected_up = subj_uplist[i][conditions[ind]].copy().pick([tailored_channel]) \n",
    "        tail_selected_down = subj_downlist[i][conditions[ind]].copy().pick([tailored_channel])\n",
    "        uptail_select_dict[conditions[ind]] =  tail_selected_up\n",
    "        downtail_select_dict[conditions[ind]] =  tail_selected_down\n",
    "        # Extract data\n",
    "        tail_up_mean_data = tail_selected_up.copy().crop(tmin=tail_tmin,tmax=tail_tmax).data\n",
    "        tail_down_mean_data = tail_selected_down.copy().crop(tmin=tail_tmin,tmax=tail_tmax).data\n",
    "        # Agragate and append\n",
    "        tailup_mean = tail_up_mean_data.mean(axis=1) * 1e6\n",
    "        taildown_mean = tail_down_mean_data.mean(axis=1) * 1e6\n",
    "        tailup_amp.append(tailup_mean[0])\n",
    "        taildown_amp.append(taildown_mean[0])\n",
    "        # RMS\n",
    "        tail_mean = np.mean([np.sqrt(tailup_mean**2),np.sqrt(taildown_mean**2)])\n",
    "        tailored_amp.append(tail_mean)\n",
    "\n",
    "        # Ultra tailored\n",
    "        # Pick channel\n",
    "        ultra_selected_up = subj_uplist[i][conditions[ind]].copy().pick([ultraup_channel]) \n",
    "        ultra_selected_down = subj_downlist[i][conditions[ind]].copy().pick([ultradown_channel])\n",
    "        upultra_select_dict[conditions[ind]] =  ultra_selected_up\n",
    "        downultra_select_dict[conditions[ind]] =  ultra_selected_down\n",
    "        # Extract data\n",
    "        ultra_up_mean_data = ultra_selected_up.copy().crop(tmin=ultraup_tmin,tmax=ultraup_tmax).data\n",
    "        ultra_down_mean_data = ultra_selected_down.copy().crop(tmin=ultradown_tmin,tmax=ultradown_tmax).data\n",
    "        # Agragate and append\n",
    "        ultraup_mean = ultra_up_mean_data.mean(axis=1) * 1e6\n",
    "        ultradown_mean = ultra_down_mean_data.mean(axis=1) * 1e6\n",
    "        ultraup_amp.append(ultraup_mean[0])\n",
    "        ultradown_amp.append(ultradown_mean[0])\n",
    "        # RMS\n",
    "        ultra_mean = np.mean([np.sqrt(ultraup_mean**2),np.sqrt(ultradown_mean**2)])\n",
    "        ultra_amp.append(ultra_mean)\n",
    "    \n",
    "    selectup_gen_evoked.append(upgen_select_dict)\n",
    "    selectdown_gen_evoked.append(downgen_select_dict) \n",
    "    selectup_tail_evoked.append(uptail_select_dict)\n",
    "    selectdown_tail_evoked.append(downtail_select_dict) \n",
    "    selectup_ultra_evoked.append(uptail_select_dict)\n",
    "    selectdown_ultra_evoked.append(downtail_select_dict) \n",
    "\n",
    "c1_long_df['attention'] = mid_att\n",
    "c1_long_df['expectation'] = mid_pred\n",
    "c1_long_df['general_amp'] = general_amp\n",
    "c1_long_df['general_up_amp'] = genup_amp\n",
    "c1_long_df['general_down_amp'] = gendown_amp\n",
    "c1_long_df['tailored_amp'] = tailored_amp\n",
    "c1_long_df['tailored_up_amp'] = tailup_amp\n",
    "c1_long_df['tailored_down_amp'] = taildown_amp\n",
    "c1_long_df['ultra_amp'] = ultra_amp\n",
    "c1_long_df['ultra_up_amp'] = ultraup_amp\n",
    "c1_long_df['ultra_down_amp'] = ultradown_amp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_long_df.to_csv(r'C:\\Users\\mvmigem\\Documents\\data\\project_1\\compiled_dataframes\\c1_df.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_up_evokeds = {}\n",
    "general_down_evokeds = {}\n",
    "\n",
    "tailor_up_evokeds = {}\n",
    "tailor_down_evokeds = {}\n",
    "\n",
    "ultra_up_evokeds = {}\n",
    "ultra_down_evokeds = {}\n",
    "\n",
    "for i,cond in enumerate(conditions):\n",
    "    list_for_upgencond = []\n",
    "    list_for_downgencond = []\n",
    "\n",
    "    list_for_uptailcond = []\n",
    "    list_for_downtailcond = []\n",
    "\n",
    "    list_for_upultracond = []\n",
    "    list_for_downultracond = []\n",
    "\n",
    "    for ind,sub in enumerate(sub_list):\n",
    "        list_for_upgencond.append(selectup_gen_evoked[i][cond])\n",
    "        list_for_downgencond.append(selectdown_gen_evoked[i][cond])\n",
    "\n",
    "        list_for_uptailcond.append(selectup_tail_evoked[i][cond])\n",
    "        list_for_downtailcond.append(selectdown_tail_evoked[i][cond])\n",
    "\n",
    "        list_for_upultracond.append(selectup_ultra_evoked[i][cond])\n",
    "        list_for_downultracond.append(selectdown_ultra_evoked[i][cond])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying common channels ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying common channels ...\n",
      "Identifying common channels ...\n",
      "Identifying common channels ...\n",
      "Identifying common channels ...\n",
      "Identifying common channels ...\n",
      "Identifying common channels ...\n",
      "Identifying common channels ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evoked_ga_up = [mne.grand_average(evokeds_up['reg_att']),\n",
    "               mne.grand_average(evokeds_up['odd_att']),\n",
    "               mne.grand_average(evokeds_up['reg_unatt']),\n",
    "               mne.grand_average(evokeds_up['odd_unatt'])]\n",
    "\n",
    "evoked_ga_down = [mne.grand_average(evokeds_down['reg_att']),\n",
    "               mne.grand_average(evokeds_down['odd_att']),\n",
    "               mne.grand_average(evokeds_down['reg_unatt']),\n",
    "               mne.grand_average(evokeds_down['odd_unatt'])]\n",
    "\n",
    "evoked_ga = []\n",
    "\n",
    "for i,(up,down) in enumerate(zip(evoked_ga_up,evoked_ga_down)):\n",
    "    rms_data = np.mean([np.sqrt(up.data**2),np.sqrt(down.data**2)],axis=0)\n",
    "    rms_evoked = mne.EvokedArray(rms_data,info=up.info, tmin=up.times[0])\n",
    "    evoked_ga.append(rms_evoked)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conditions = ('attended reg','attended odd','unattended reg','unattended odd')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Figure size 800x600 with 2 Axes>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "norm = dict(zip(conditions,evoked_ga))\n",
    "# norm = general_down_evokeds\n",
    "epoch_set1 = norm\n",
    "scale = [-6, 6]\n",
    "# mne.viz.plot_compare_evokeds(epoch_set1, picks= 'Pz', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1,picks=['POz'],vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "# mne.viz.plot_compare_evokeds(epoch_set1, picks= 'Oz', vlines=[0.05,0.1],ylim=dict(eeg=scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tailor_up_evokeds['reg_att'].plot(ylim=dict(eeg=scale))\n",
    "tailor_up_evokeds['odd_att'].plot(ylim=dict(eeg=scale))\n",
    "tailor_up_evokeds['reg_unatt'].plot(ylim=dict(eeg=scale))\n",
    "tailor_up_evokeds['odd_unatt'].plot(ylim=dict(eeg=scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'C1', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'C2', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'Cz', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'P1', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'P2', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'Pz', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'PO3', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'PO4', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'POz', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'O1', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'O2', vlines=[0.05,0.1],ylim=dict(eeg=scale))\n",
    "mne.viz.plot_compare_evokeds(epoch_set1, picks= 'Oz', vlines=[0.05,0.1],ylim=dict(eeg=scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "times = np.linspace(0.1, 0.5, 7)\n",
    "vlim=(-3, 3)\n",
    "\n",
    "evoked_pred[0].plot_topomap(ch_type=\"eeg\", times=times, colorbar=True, vlim=vlim)\n",
    "evoked_pred[1].plot_topomap(ch_type=\"eeg\", times=times, colorbar=True, vlim=vlim)\n",
    "evoked_pred[2].plot_topomap(ch_type=\"eeg\", times=times, colorbar=True, vlim=vlim)\n",
    "evoked_pred[3].plot_topomap(ch_type=\"eeg\", times=times, colorbar=True, vlim=vlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.linspace(0.05, 0.11, 5)\n",
    "group4_pos1_unatt.plot_topomap(ch_type=\"eeg\", times=times, colorbar=True)\n",
    "group4_pos3_att.plot_topomap(ch_type=\"eeg\", times=times, colorbar=True)\n",
    "group4_pos1_unatt.plot_topomap(ch_type=\"eeg\", times=times, colorbar=True)\n",
    "group4_pos3_att.plot_topomap(ch_type=\"eeg\", times=times, colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time course of c1 peaks and means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Time course first sequence position\n",
    "\"\"\"\n",
    "# attended\n",
    "ep_df_att_1 = ep_att_1.metadata\n",
    "ep_df_att_1['poz_c1_mean'] = ep_att_1.crop(tmin=.07, tmax=.1).get_data(picks='POz').mean(axis=2)\n",
    "ep_df_att_2 = ep_att_2.metadata\n",
    "ep_df_att_2['poz_c1_mean'] = ep_att_2.crop(tmin=.07, tmax=.1).get_data(picks='POz').mean(axis=2)\n",
    "ep_df_att_3 = ep_att_3.metadata\n",
    "ep_df_att_3['poz_c1_mean'] = ep_att_3.crop(tmin=.07, tmax=.1).get_data(picks='POz').mean(axis=2)\n",
    "ep_df_att_4 = ep_att_4.metadata\n",
    "ep_df_att_4['poz_c1_mean'] = ep_att_4.crop(tmin=.07, tmax=.1).get_data(picks='POz').mean(axis=2)\n",
    "# unattend\n",
    "ep_df_unatt_1= ep_unatt_1.metadata\n",
    "ep_df_unatt_1['poz_c1_mean'] = ep_unatt_1.crop(tmin=.07, tmax=.1).get_data(picks='POz').mean(axis=2)\n",
    "ep_df_unatt_2 = ep_unatt_2.metadata\n",
    "ep_df_unatt_2['poz_c1_mean'] = ep_unatt_2.crop(tmin=.07, tmax=.1).get_data(picks='POz').mean(axis=2)\n",
    "ep_df_unatt_3 = ep_unatt_3.metadata\n",
    "ep_df_unatt_3['poz_c1_mean'] = ep_unatt_3.crop(tmin=.07, tmax=.1).get_data(picks='POz').mean(axis=2)\n",
    "ep_df_unatt_4 = ep_unatt_4.metadata\n",
    "ep_df_unatt_4['poz_c1_mean'] = ep_unatt_4.crop(tmin=.07, tmax=.1).get_data(picks='POz').mean(axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Time course\n",
    "\"\"\"\n",
    "position = ['pos1','pos2','pos3','pos4']\n",
    "condition = 'attention == \"attended\" & expected == \"regular\" & start_position == 2'\n",
    "pos1_win_data = c1_window_epochs[condition][position[0]].get_data(picks='POz')\n",
    "pos2_win_data = c1_window_epochs[condition][position[1]].get_data(picks='POz')\n",
    "pos3_win_data = c1_window_epochs[condition][position[2]].get_data(picks='POz')\n",
    "pos4_win_data = c1_window_epochs[condition][position[3]].get_data(picks='POz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_c1_1 = pos1_win_data.mean(axis=2).squeeze()\n",
    "mean_c1_2 = pos2_win_data.mean(axis=2).squeeze()\n",
    "mean_c1_3 = pos3_win_data.mean(axis=2).squeeze()\n",
    "mean_c1_4 = pos4_win_data.mean(axis=2).squeeze()\n",
    "\n",
    "max_c1_1 = pos1_win_data.max(axis=2).squeeze()\n",
    "max_c1_2 = pos2_win_data.max(axis=2).squeeze()\n",
    "max_c1_3 = pos3_win_data.max(axis=2).squeeze()\n",
    "max_c1_4 = pos4_win_data.max(axis=2).squeeze()\n",
    "\n",
    "mean_c1_list = [mean_c1_1,mean_c1_2,mean_c1_3,mean_c1_4]\n",
    "max_c1_list = [max_c1_1,max_c1_2,max_c1_3,max_c1_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting mean c1 voltage time wise\n",
    "\"\"\"\n",
    "plt.rcParams[\"figure.figsize\"] = [18,10]\n",
    "fig, axs = plt.subplots(4,2, sharey=True)\n",
    "\n",
    "sns.regplot(data = ep_df_att_1, x = 'trial', y = 'poz_c1_mean', ax = axs[0,0],scatter_kws={'s': 10, 'color': 'red'})\n",
    "sns.regplot(data = ep_df_unatt_1, x = 'trial', y = 'poz_c1_mean', ax = axs[1,0],scatter_kws={'s': 10, 'color': 'green'})\n",
    "sns.regplot(data = ep_df_att_2, x = 'trial', y = 'poz_c1_mean', ax = axs[0,1],scatter_kws={'s': 10, 'color': 'red'})\n",
    "sns.regplot(data = ep_df_unatt_2, x = 'trial', y = 'poz_c1_mean', ax = axs[1,1],scatter_kws={'s': 10, 'color': 'green'})\n",
    "sns.regplot(data = ep_df_att_3, x = 'trial', y = 'poz_c1_mean', ax = axs[2,1],scatter_kws={'s': 10, 'color': 'red'})\n",
    "sns.regplot(data = ep_df_unatt_3, x = 'trial', y = 'poz_c1_mean', ax = axs[3,1],scatter_kws={'s': 10, 'color': 'green'})\n",
    "sns.regplot(data = ep_df_att_4, x = 'trial', y = 'poz_c1_mean', ax = axs[2,0],scatter_kws={'s': 10, 'color': 'red'})\n",
    "sns.regplot(data = ep_df_unatt_4, x = 'trial', y = 'poz_c1_mean', ax = axs[3,0],scatter_kws={'s': 10, 'color': 'green'})\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# sns.regplot(x = np.arange(len(mean_c1_list[1])), y = mean_c1_list[1], ax=axs[0,1])\n",
    "# sns.regplot(x = np.arange(len(mean_c1_list[2])), y = mean_c1_list[2], ax=axs[1,1])\n",
    "# sns.regplot(x = np.arange(len(mean_c1_list[3])), y = mean_c1_list[3], ax=axs[1,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting max c1 voltage time wise\n",
    "\"\"\"\n",
    "plt.rcParams[\"figure.figsize\"] = [18,10]\n",
    "fig, axs = plt.subplots(2,2, sharey=True)\n",
    "\n",
    "sns.regplot(x = np.arange(len(max_c1_list[0])), y = max_c1_list[0], ax=axs[0,0])\n",
    "sns.regplot(x = np.arange(len(max_c1_list[1])), y = max_c1_list[1], ax=axs[0,1])\n",
    "sns.regplot(x = np.arange(len(max_c1_list[2])), y = max_c1_list[2], ax=axs[1,1])\n",
    "sns.regplot(x = np.arange(len(max_c1_list[3])), y = max_c1_list[3], ax=axs[1,0])\n",
    "\n",
    "plt.suptitle('max values '+ condition)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time frequency exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_first_att = epochs_attended['pos4/seq2'].average().compute_psd()\n",
    "spec_last_att = epochs_attended['seq3'].average().compute_psd()\n",
    "spec_first_unatt = epochs_unattended['seq1'].average().compute_psd()\n",
    "spec_last_unatt = epochs_unattended['seq3'].average().compute_psd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_first_att.plot_topomap(ch_type=\"eeg\")\n",
    "spec_first_unatt.plot_topomap(ch_type=\"eeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = np.logspace(*np.log10([8, 13]), num=8)\n",
    "n_cycles = freqs / 3.0  # different number of cycle per frequency\n",
    "power, itc = tfr_morlet(\n",
    "    epochs_attended['pos4/seq2'],\n",
    "    freqs=freqs,\n",
    "    n_cycles=n_cycles,\n",
    "    use_fft=True,\n",
    "    return_itc=True,\n",
    "    n_jobs=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "power.plot([27], baseline=(-0.09, 0), mode=\"logratio\", title=power.ch_names[27])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(7, 4), layout=\"constrained\")\n",
    "topomap_kw = dict(\n",
    "    ch_type=\"eeg\", tmin=0.09, tmax=0.5, baseline=(-0.09, 0), mode=\"logratio\", show=False\n",
    ")\n",
    "plot_dict = dict(Alpha=dict(fmin=8, fmax=13))\n",
    "for ax, (title, fmin_fmax) in zip(axes, plot_dict.items()):\n",
    "    power.plot_topomap(**fmin_fmax, axes=ax, **topomap_kw)\n",
    "    ax.set_title(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
